---
title: "Machine Learning Intro"
subtitle: "Fall R '23"
author: "Dominic Bordelon, Research Data Librarian, ULS"
format: 
  revealjs:
    self-contained: true
    footer: "R 5: Machine Learnin Intro"
    logo: "images/ULS_logo1.png"
    code-copy: true
editor: visual
execute:
  echo: true
  eval: false
---

```{r}
#| output: false
#| echo: false
#| eval: true
library(tidyverse)
library(tidymodels)
library(e1071)
```

## Agenda {.smaller}

## About the trainer {.smaller}

**Dominic Bordelon, Research Data Librarian**\
University Library System, University of Pittsburgh\
[dbordelon\@pitt.edu](mailto:dbordelon@pitt.edu)

::: columns
::: {.column width="50%"}
Services for the Pitt community:

-   Consultations
-   Training (on-request and via public workshops)
-   Talks (on-request and publicly)
-   Research collaboration
:::

::: {.column width="50%"}
Support areas and interests:

-   Computer programming fundamentals, esp. for data processing and analysis
-   Open Science and Data Sharing
-   Data stewardship/curation
-   Research methods; science and technology studies
:::
:::

## Fall R Series

| \#    | Date     | Title                             |
|-------|----------|-----------------------------------|
| 1     | 8/29     | Getting Started with Tabular Data |
| 2     | 9/5      | Working with Data Frames          |
| 3     | 9/12     | Data Visualization                |
| 4     | 9/19     | Inference and Modeling Intro      |
| **5** | **9/26** | **Machine Learning Intro**        |

## 

![](images/r-office-hour-fall-23.png){fig-alt="Advertisement for the R Drop-in Hour service" fig-align="center"}

## Today's packages

::: columns
::: {.column width="50%"}
`tidymodels`, particularly:

-   `broom`: tidy model representation
-   `parsnip`: standardized modeling interface

```{r}
install.packages(c("tidymodels", "infer"))
```
:::

::: {.column width="50%"}
![](images/parsnip-logo.png){width="150"}
:::
:::

## ...and using penguins examples

```{r}
#| eval: false
#| echo: true

library(palmerpenguins)
data(penguins)
names(penguins)
```

```{r}
#| eval: true


```

# What is Machine Learning?

## Machine learning

Combines Computer Science and Statistics to analyze data in a variety of ways

Often involves fitting a model to "training data"; the model is then validated against "test data" (partitioned prior to fitting)

‚ö† But not all ML methods can be "validated" (see: unsupervised methods)

A model may built and run once to form an analysis; or the model may be embedded in a larger software package where it continually generates predictions/classifications from new data

May be applied in "generative" settings such as a chat bot

## 

![A widely adopted conceptual framework of data science competencies, illustrating where ML fits. Image source: Andrew Silver, ["The Essential Data Science Venn Diagram"](https://towardsdatascience.com/the-essential-data-science-venn-diagram-35800c3bef40) (2018)](images/silver-venn.jpg){fig-align="center"}

Definitions

Applications

## Trends you'll see in ML {.smaller}

-   Validation ("training" and "test" sets), iteration, randomness, and model selection
-   Resampling methods (bootstrap)
-   High-dimensional data (many predictors/columns), e.g, genetic data
    -   "Curse of dimensionality": regression fails in $p$-dimensional space as $p$ (number of predictors) approaches and surpasses $n$, because the space is too sparsely populated to avoid overfitting.
-   Accuracy/Explainability tradeoff
-   Many ML methods require a large sample size, compared to classical statistics; methods (as currently practiced) tend to be more naive compared to domain-embedded statistics, but may also leverage compute to a much greater degree
-   "Ensemble methods" which combine many simple models to form a single, powerful model
-   Application of Occam's Razor (simplest is best) when confronted with similar results from different models
-   ML "black box" methods can be useful for exploration even if they cannot always answer research questions

## 

![The Curse of Dimensionality. Note the number of total regions in each space and the relative sparseness of the 3D space ($p$ = 3) with $n$ = 7 observations. Image source: [Thudumu et al. 2020](https://doi.org/10.1186/s40537-020-00320-x)](images/dimensionality.jpg){fig-align="center"}

## Common ML problem applications

-   Prediction: based on what we've seen before, what $\hat{Y}$ do we predict from $X_1 ‚Ä¶ X_p$ inputs?
    -   Note: As in classical statistics, may also be used for inference/explanation rather than prediction (what are the covariate relationships in the observed data?)
-   Classification: based on what we've seen before, what kind of thing is this?
-   Clustering: looking at the data, what clusters exist?
-   Dimension reduction: how can I reduce the number of predictors, while minimizing the information lost?
-   Anomaly detection: what is unusual? (important for medical settings!)
-   Semantic analysis: what does this mean? (e.g., text analysis, computer vision)
-   Predictive media generation

## ML terminology

Often describes statistical concepts with different language, due to separate disciplinary traditions.

| Statistics term                       | ML / Computer Science term              |
|-----------------------------------|-------------------------------------|
| observation                           | example, instance                       |
| response variable, dependent variable | label, output                           |
| predictor, independent variable       | feature, input                          |
| regression                            | regression, supervised learner, machine |
| estimation                            | learning                                |

: Some terminology encountered in ML and statistics. Source: Adapted from Zachary Kurtz, ["Translating Between Statistics and Machine Learning"](https://insights.sei.cmu.edu/blog/translating-between-statistics-and-machine-learning/) (2018)

‚ö† Terms/concepts to be careful with in ML, coming from stats:

-   hypothesis (sometimes an output of a classifier model)
-   bias (broader meaning)
-   causality (sometimes less rigorous than stats)

## Major ML approaches {.smaller}

-   **Supervised learning**: predictive; there is a $Y$ (output)
    -   ("Supervised" because we can *supervise* the process with a target, and assess the model's accuracy)
    -   **Regression**, e.g., (multiple) linear, logistic, polynomial, splines
    -   **Decision trees** and random forests
    -   Linear discriminant analysis (LDA), a classifier which fits a linear boundary
    -   **Naive Bayes classifier**
    -   Support vector machines (SVMs), binary classifiers with nonlinear class boundaries
    -   Artificial neural networks (ANNs), which are *deep learning* when sufficiently layered
-   **Unsupervised learning**: there is no response $Y$ (output) against which to validate; exploratory
    -   Clustering techniques
    -   Probability density estimation (i.e., inferring distribution)
    -   Principal Components Analysis (PCA), a dimensionality reducing method
-   Reinforcement learning: the model receives positive/negative feedback to outputs and learns accordingly

We will take a closer look at the ones in bold.

\[VISUAL\]

# Regression

## Linear regression

Linear regression can answer: (James et al. 2021)

1.  Is there a relationship between $\beta_{1...p}$ and $Y$?

2.  How strong is the relationship?

3.  Which subgroups have the strongest relationship?

4.  How accurately can we predict future $Y$ from future $\beta_{1...p}$?

5.  Is the relationship linear?

6.  Are there interaction effects among $\beta_{1‚Ä¶p}$?

## Multiple linear regression {.smaller}

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon
$$

We are trying to estimate each of the $\hat{\beta}_0...\hat{\beta}_p$ coefficients for the above model. While simple linear regression, $p$ = 2, is on a 2D plane, the multiple linear regression is in a $p$-dimensional space.

We still use the `lm()` function, adding more predictors with `+` . Given a factor (qualitative variable), dummy variables will be created automatically.

```{r}
data(penguins)
p_bill_fit <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(bill_length_mm ~ body_mass_g + flipper_length_mm + bill_depth_mm, 
      data = penguins)
summary(p_bill_fit$fit)

# with factors; note dummy variables in summary:
p_bill_fit <- linear_reg() %>% 
  fit(bill_length_mm ~ body_mass_g + flipper_length_mm + bill_depth_mm + species + sex,
      data = penguins)
summary(p_bill_fit$fit)
```

## Writing formulas in R

`bill_length_mm ~ body_mass_g` is an example of a **formula** in R.

```{r}
?formula
```

## Logistic regression

We assign the two levels of a binomial factor to a probability space, where 0 is level A of a factor, and 1 is level B of a factor. Then we a fit a model (often linear). For some $X$, $Y$ is ultimately interpreted as either 0 or 1, depending on whether the probability lies above or below 0.5.

```{r}
penguins_clean <- penguins %>% 
  filter(species %in% c("Gentoo", "Adelie"),
         !is.na(species),
         !is.na(body_mass_g)) %>% 
  mutate(species = fct_drop(species))
species_fit <- logistic_reg() %>% 
  set_engine("glm") %>% 
  fit(species ~ body_mass_g, data = penguins_clean)
summary(species_fit$fit)

ggplot(penguins_clean, aes(x=body_mass_g, y=species)) + geom_point() + 
  geom_line(aes(y=predict(species_fit$fit)), color="blue") +
  coord_cartesian(ylim=c(1, 2))

```

# Decision trees

## Decision trees

-   Segment the predictor space into regions with if/else boundaries
-   To predict: return mean/mode response $Y$ for the region of which $X$ is a member
-   üëç Very interpretable, but üëé tend to be less accurate than regression methods
-   *Random forests* and similar methods build and prune very many decision trees randomly and then form a single model, improving accuracy and mitigating overfitting
-   May be used for classification or regression; non-parametric

```{r}
library(palmerpenguins)
data(penguins)
library(tidyverse)
library(tidymodels)
library(rpart.plot) 


```

# Naive Bayes classifier

## Naive Bayes classifier {.smaller}

Uses Bayes' theorem

$$
Pr(Y = k|X = x) = \frac{\pi_kf_k(x)}{\sum^K_{l=1} \pi_lf_l(x)}
$$

where $\pi_k$ is the overall *prior* probability that an observation comes from $k$ class, and $f_k$ is the density function. In estimating $f_1(x),...,f_K(x)$, we assume only: ***Within the kth class, the p predictors are independent*****.** For $k = 1,‚Ä¶,K$,

$$
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times ... \times f_{kp}(x_p)
$$

Density estimation is simply treated as Gaussian for each predictor. These are probably unrealistic assumptions---hence *naive*---but such models nevertheless return "decent" results (James et al. 2021) without the challenges of estimating joint distributions.

The classifier model calculates a probability, which it then uses to assign a class (i.e., output factor level) according to a threshold (tuneable for application).

```{r}
penguins_clean <- penguins %>% 
  filter(species %in% c("Gentoo", "Adelie"),
         !is.na(species),
         !is.na(body_mass_g)) %>% 
  mutate(species = fct_drop(species))

species_fit <- naiveBayes(species ~ body_mass_g + flipper_length_mm, data = penguins_clean)

species_fit
predict(species_fit, data.frame(body_mass_g=3500, flipper_length_mm=22))
```

## Model *responsibly* {.smaller}

-   Consider the validity and reliability of your measures
-   Correlation $\neq$ causation
-   Always keep in mind the assumptions made by your chosen method/model
-   Beware bias and overfitting
-   Seek explainability (AKA interpretability), not merely predictive accuracy
-   Remember the Data Science Danger Zones! üíÄ (computers are not magic)

## Recommended reading

::: columns
::: {.column width="50%"}
[![](images/james-isl.jpg){fig-align="center"}](https://www.statlearning.com/)

James et al. 2021

Full text available free at <https://www.statlearning.com/>
:::

::: {.column width="50%"}
[![](images/lantz-ml.jpg){fig-align="center"}](https://pitt.primo.exlibrisgroup.com/permalink/01PITT_INST/i25aoe/cdi_safari_books_v2_9781801071321)

Lantz 2023

Available in [PittCat via O'Reilly Online Learning](https://pitt.primo.exlibrisgroup.com/permalink/01PITT_INST/i25aoe/cdi_safari_books_v2_9781801071321) (requires Pitt Passport)
:::
:::

## or, looking for discipline-specific R?

Check out the Big Book of R! An online directory at <https://www.bigbookofr.com/> of very many R ebooks, most of them free OER and produced by experts, organized by discipline/topic and searchable.

Look up your discipline (or some topic that interests you, e.g., time series data) and see what applications of R you can find.

[![Example graphic of a recent update](images/big-book.png){fig-align="center"}](https://www.bigbookofr.com/)

# Wrap up

## Session in review

Today we learned about:

-   what machine learning is, esp. relative to familiar concepts
